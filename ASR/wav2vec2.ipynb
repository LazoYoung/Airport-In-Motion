{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a8edc1ffea0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# required to sample audio\n",
    "!pip install librosa soundfile pyaudio\n",
    "\n",
    "!pip install ipywidgets\n",
    "\n",
    "# required for Wav2Vec2ProcessorWithLM\n",
    "# !pip install pyctcdecode\n",
    "# !pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "id": "75debb67eed3825d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T02:03:36.296865Z",
     "start_time": "2024-04-19T02:03:36.224230Z"
    }
   },
   "source": [
    "from pyaudio import PyAudio\n",
    "\n",
    "p = PyAudio()\n",
    "\n",
    "# print(\"Available input devices:\")\n",
    "# \n",
    "# for idx in range(p.get_device_count()):\n",
    "#     device = p.get_device_info_by_index(idx)\n",
    "#     if device[\"maxInputChannels\"] > 0 and device[\"maxOutputChannels\"] == 0:\n",
    "#         print(device)\n",
    "    \n",
    "device = p.get_default_input_device_info()\n",
    "device_idx = int(device[\"index\"])\n",
    "device_sample_rate = int(device[\"defaultSampleRate\"])\n",
    "print(\"Default input device:\", device_idx)\n",
    "p.terminate()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default input device: 1\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "652f52841419332c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T02:06:01.837664Z",
     "start_time": "2024-04-19T02:05:55.475654Z"
    }
   },
   "source": [
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "from threading import Thread, Lock\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchaudio.functional as F\n",
    "\n",
    "from transformers import AutoModelForCTC, Wav2Vec2Processor\n",
    "from queue import Queue\n",
    "from pyaudio import PyAudio, paInt16, Stream\n",
    "\n",
    "CHANNELS = 1\n",
    "SAMPLE_RATE = 16000\n",
    "RECORD_SECONDS = 30\n",
    "CHUNK = 1024\n",
    "AUDIO_FORMAT = paInt16\n",
    "SAMPLE_SIZE = 2\n",
    "MODEL_ID = \"Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc-and-atcosim\"\n",
    "model = AutoModelForCTC.from_pretrained(MODEL_ID)\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "recordings = Queue()\n",
    "output = widgets.Output()\n",
    "\n",
    "def is_buffer_full(buffer, chunk):\n",
    "    return len(buffer) >= (SAMPLE_RATE * RECORD_SECONDS) / chunk\n",
    "\n",
    "def process(sample: bytes):\n",
    "    nparray = np.frombuffer(sample, dtype=np.int16).astype(np.float32) / 32767.0\n",
    "    tensor = torch.from_numpy(nparray)\n",
    "    resampled_audio = F.resample(tensor, device_sample_rate, SAMPLE_RATE).numpy()\n",
    "    input_values = processor(resampled_audio, sampling_rate=SAMPLE_RATE, return_tensors=\"pt\").input_values\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "        pred_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(pred_ids)\n",
    "    \n",
    "    return transcription[0]\n",
    "\n",
    "class RecordState:\n",
    "    _state = False\n",
    "    _lock = Lock()\n",
    "    \n",
    "    def is_recording(self):\n",
    "        return self._state\n",
    "    \n",
    "    def set_recording(self, mode: bool):\n",
    "        self._lock.acquire()\n",
    "        self._state = mode\n",
    "        self._lock.release()\n",
    "\n",
    "state = RecordState()\n",
    "\n",
    "record_btn = widgets.Button(\n",
    "    description=\"Record\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    icon=\"microphone\",\n",
    ")\n",
    "stop_btn = widgets.Button(\n",
    "    description=\"Stop\",\n",
    "    disabled=False,\n",
    "    button_style=\"warning\",\n",
    "    icon=\"stop\",\n",
    ")\n",
    "\n",
    "def start_recording(data):\n",
    "    audio = PyAudio()\n",
    "    stream = audio.open(\n",
    "            format=AUDIO_FORMAT,\n",
    "            channels=CHANNELS,\n",
    "            rate=device_sample_rate,\n",
    "            frames_per_buffer=CHUNK,\n",
    "            input=True,\n",
    "            input_device_index=device_idx,\n",
    "        )\n",
    "    record_thread = Thread(target=record, args=(audio, stream,))\n",
    "    transcribe_thread = Thread(target=transcribe_loop)\n",
    "    output.append_stdout(\"Recording...\\n\")\n",
    "    state.set_recording(True)\n",
    "    record_thread.start()\n",
    "    transcribe_thread.start()\n",
    "        \n",
    "def stop_recording(data):\n",
    "    state.set_recording(False)\n",
    "        \n",
    "# chunk defines how often we read the microphone\n",
    "def record(audio: PyAudio, stream: Stream):\n",
    "    buffer = []\n",
    "    \n",
    "    while state.is_recording():\n",
    "        data = stream.read(CHUNK, True)\n",
    "        buffer.append(data)\n",
    "        \n",
    "        if is_buffer_full(buffer, CHUNK):\n",
    "            recordings.put(buffer.copy())\n",
    "            buffer = []\n",
    "            \n",
    "    if buffer:\n",
    "        recordings.put(buffer.copy())\n",
    "        transcribe()\n",
    "    \n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "def transcribe_loop():\n",
    "    while state.is_recording():\n",
    "        transcribe()\n",
    "        \n",
    "def transcribe():\n",
    "    frames = recordings.get()\n",
    "    binary = b''.join(frames)\n",
    "    text = process(binary)\n",
    "    output.append_stdout(text + '\\n')\n",
    "        \n",
    "record_btn.on_click(start_recording)\n",
    "stop_btn.on_click(stop_recording)\n",
    "\n",
    "display(record_btn, stop_btn, output)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc-and-atcosim were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at Jzuluaga/wav2vec2-large-960h-lv60-self-en-atc-uwb-atcc-and-atcosim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Button(button_style='success', description='Record', icon='microphone', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b56d4931d1f248ca95ee7cf2bb1841b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Button(button_style='warning', description='Stop', icon='stop', style=ButtonStyle())"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cc0554dc9944124b956e0986480473a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Output()"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86dbebeef86e44ab834f8e101c640bcf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
